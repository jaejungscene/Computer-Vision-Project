{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gmk_0\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: 24,025,600\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'base_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 214\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m):\n\u001b[0;32m    213\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[1;32m--> 214\u001b[0m     np_img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(base_dir,\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimages/\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    215\u001b[0m     np_img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(np_img, cv2\u001b[39m.\u001b[39mCOLOR_RGB2BGR)\n\u001b[0;32m    216\u001b[0m     \u001b[39m# np_img = np.array(img)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'base_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torchvision import transforms\n",
    "\n",
    "# # base_dir = os.path.dirname(__file__)\n",
    "# base_dir = \"C:/Users/gmk_0/source/repos/pythonProject/IT2/Computer-Vision-Project\"\n",
    "# __all__ = ['iresnet18', 'iresnet34', 'iresnet50', 'iresnet100', 'iresnet200']\n",
    "# using_ckpt = False\n",
    "\n",
    "# def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "#     \"\"\"3x3 convolution with padding\"\"\"\n",
    "#     return nn.Conv2d(in_planes,\n",
    "#                      out_planes,\n",
    "#                      kernel_size=3,\n",
    "#                      stride=stride,\n",
    "#                      padding=dilation,\n",
    "#                      groups=groups,\n",
    "#                      bias=False,\n",
    "#                      dilation=dilation)\n",
    "\n",
    "\n",
    "# def conv1x1(in_planes, out_planes, stride=1):\n",
    "#     \"\"\"1x1 convolution\"\"\"\n",
    "#     return nn.Conv2d(in_planes,\n",
    "#                      out_planes,\n",
    "#                      kernel_size=1,\n",
    "#                      stride=stride,\n",
    "#                      bias=False)\n",
    "\n",
    "\n",
    "# class IBasicBlock(nn.Module):\n",
    "#     expansion = 1\n",
    "#     def __init__(self, inplanes, planes, stride=1, downsample=None,\n",
    "#                  groups=1, base_width=64, dilation=1):\n",
    "#         super(IBasicBlock, self).__init__()\n",
    "#         if groups != 1 or base_width != 64:\n",
    "#             raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "#         if dilation > 1:\n",
    "#             raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "#         self.bn1 = nn.BatchNorm2d(inplanes, eps=1e-05,)\n",
    "#         self.conv1 = conv3x3(inplanes, planes)\n",
    "#         self.bn2 = nn.BatchNorm2d(planes, eps=1e-05,)\n",
    "#         self.prelu = nn.PReLU(planes)\n",
    "#         self.conv2 = conv3x3(planes, planes, stride)\n",
    "#         self.bn3 = nn.BatchNorm2d(planes, eps=1e-05,)\n",
    "#         self.downsample = downsample\n",
    "#         self.stride = stride\n",
    "\n",
    "#     def forward_impl(self, x):\n",
    "#         identity = x\n",
    "#         out = self.bn1(x)\n",
    "#         out = self.conv1(out)\n",
    "#         out = self.bn2(out)\n",
    "#         out = self.prelu(out)\n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn3(out)\n",
    "#         if self.downsample is not None:\n",
    "#             identity = self.downsample(x)\n",
    "#         out += identity\n",
    "#         return out        \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if self.training and using_ckpt:\n",
    "#             return checkpoint(self.forward_impl, x)\n",
    "#         else:\n",
    "#             return self.forward_impl(x)\n",
    "\n",
    "\n",
    "# class IResNet(nn.Module):\n",
    "#     fc_scale = 7 * 7\n",
    "#     def __init__(self,\n",
    "#                  block, layers, dropout=0, num_features=512, zero_init_residual=False,\n",
    "#                  groups=1, width_per_group=64, replace_stride_with_dilation=None, fp16=False):\n",
    "#         super(IResNet, self).__init__()\n",
    "#         self.extra_gflops = 0.0\n",
    "#         self.fp16 = fp16\n",
    "#         self.inplanes = 64\n",
    "#         self.dilation = 1\n",
    "#         if replace_stride_with_dilation is None:\n",
    "#             replace_stride_with_dilation = [False, False, False]\n",
    "#         if len(replace_stride_with_dilation) != 3:\n",
    "#             raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "#                              \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "#         self.groups = groups\n",
    "#         self.base_width = width_per_group\n",
    "#         self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(self.inplanes, eps=1e-05)\n",
    "#         self.prelu = nn.PReLU(self.inplanes)\n",
    "#         self.layer1 = self._make_layer(block, 64, layers[0], stride=2)\n",
    "#         self.layer2 = self._make_layer(block,\n",
    "#                                        128,\n",
    "#                                        layers[1],\n",
    "#                                        stride=2,\n",
    "#                                        dilate=replace_stride_with_dilation[0])\n",
    "#         self.layer3 = self._make_layer(block,\n",
    "#                                        256,\n",
    "#                                        layers[2],\n",
    "#                                        stride=2,\n",
    "#                                        dilate=replace_stride_with_dilation[1])\n",
    "#         self.layer4 = self._make_layer(block,\n",
    "#                                        512,\n",
    "#                                        layers[3],\n",
    "#                                        stride=2,\n",
    "#                                        dilate=replace_stride_with_dilation[2])\n",
    "#         self.bn2 = nn.BatchNorm2d(512 * block.expansion, eps=1e-05,)\n",
    "#         self.dropout = nn.Dropout(p=dropout, inplace=True)\n",
    "#         self.fc = nn.Linear(512 * block.expansion * self.fc_scale, num_features)\n",
    "#         self.features = nn.BatchNorm1d(num_features, eps=1e-05)\n",
    "#         nn.init.constant_(self.features.weight, 1.0)\n",
    "#         self.features.weight.requires_grad = False\n",
    "\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 nn.init.normal_(m.weight, 0, 0.1)\n",
    "#             elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "#                 nn.init.constant_(m.weight, 1)\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "\n",
    "#         if zero_init_residual:\n",
    "#             for m in self.modules():\n",
    "#                 if isinstance(m, IBasicBlock):\n",
    "#                     nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "#     def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "#         downsample = None\n",
    "#         previous_dilation = self.dilation\n",
    "#         if dilate:\n",
    "#             self.dilation *= stride\n",
    "#             stride = 1\n",
    "#         if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "#             downsample = nn.Sequential(\n",
    "#                 conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "#                 nn.BatchNorm2d(planes * block.expansion, eps=1e-05, ),\n",
    "#             )\n",
    "#         layers = []\n",
    "#         layers.append(\n",
    "#             block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "#                   self.base_width, previous_dilation))\n",
    "#         self.inplanes = planes * block.expansion\n",
    "#         for _ in range(1, blocks):\n",
    "#             layers.append(\n",
    "#                 block(self.inplanes,\n",
    "#                       planes,\n",
    "#                       groups=self.groups,\n",
    "#                       base_width=self.base_width,\n",
    "#                       dilation=self.dilation))\n",
    "\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # with torch.cuda.amp.autocast(self.fp16):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.prelu(x)\n",
    "#         x = self.layer1(x)\n",
    "#         # print(\"-\"*50)\n",
    "#         # print(x.shape)\n",
    "#         x = self.layer2(x)\n",
    "#         # print(x.shape)\n",
    "#         x = self.layer3(x)\n",
    "#         # print(x.shape)\n",
    "#         x = self.layer4(x)\n",
    "#         # print(x.shape)\n",
    "#         x = self.bn2(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.dropout(x)\n",
    "#         # print(x.shape)\n",
    "#         x = self.fc(x.float() if self.fp16 else x)\n",
    "#         # print(x.shape)\n",
    "#         x = self.features(x)\n",
    "#         # print(x.shape)\n",
    "#         # print(\"-\"*50)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# def _iresnet(arch, block, layers, pretrained, progress, **kwargs):\n",
    "#     model = IResNet(block, layers, **kwargs)\n",
    "#     if pretrained:\n",
    "#         raise ValueError()\n",
    "#     return model\n",
    "# def iresnet18(pretrained=False, progress=True, **kwargs):\n",
    "#     return _iresnet('iresnet18', IBasicBlock, [2, 2, 2, 2], pretrained,\n",
    "#                     progress, **kwargs)\n",
    "# def iresnet34(pretrained=False, progress=True, **kwargs):\n",
    "#     return _iresnet('iresnet34', IBasicBlock, [3, 4, 6, 3], pretrained,\n",
    "#                     progress, **kwargs)\n",
    "# def iresnet50(pretrained=False, progress=True, **kwargs):\n",
    "#     return _iresnet('iresnet50', IBasicBlock, [3, 4, 14, 3], pretrained,\n",
    "#                     progress, **kwargs)\n",
    "\n",
    "\n",
    "# back = torch.load(os.path.join(base_dir,\"weights/backbone-r18.pth\"), map_location=torch.device(\"cpu\"))\n",
    "# model = iresnet18()\n",
    "# model.load_state_dict(back)\n",
    "\n",
    "# print(\"params: {:,}\".format(sum([p.data.nelement() for p in model.parameters()])))\n",
    "# print(model)\n",
    "# inputs = torch.randn((1,3,112,112))\n",
    "# img = Image.open(os.path.join(base_dir,\"images/jaejung.jpg\"))\n",
    "\n",
    "from model import get_model\n",
    "model = get_model()\n",
    "\n",
    "vector_list = []\n",
    "for i in range(4):\n",
    "    print(\"-\"*50)\n",
    "    np_img = cv2.imread(os.path.join(base_dir,f\"images/{i}.jpg\"))\n",
    "    np_img = cv2.cvtColor(np_img, cv2.COLOR_RGB2BGR)\n",
    "    # np_img = np.array(img)\n",
    "    if np_img.shape[-1] > 3:\n",
    "        print(\"alpha channel remove\")\n",
    "        np_img = np_img[:,:,0:3]\n",
    "\n",
    "    print(np_img.shape)\n",
    "    plt.imshow(np_img)\n",
    "    plt.show()\n",
    "\n",
    "    max_size = max(np_img.shape[0:-1])\n",
    "    min_size = min(np_img.shape[0:-1])\n",
    "    image_transforms = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.CenterCrop((min_size,min_size)),\n",
    "            transforms.Resize(size=(112,112)), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                [0.485, 0.456, 0.406], \n",
    "                [0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    torch_img = image_transforms(np_img)\n",
    "    C, H, W = torch_img.size()\n",
    "    torch_img = torch_img.view(1,C,H,W)\n",
    "    print(torch_img.shape)\n",
    "    plt.imshow(torch_img.view(C,H,W).permute(1,2,0))\n",
    "    plt.show()\n",
    "    print(\"inputs shape:\", torch_img.shape)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = model(torch_img)\n",
    "    print(\"outputs shape:\", outputs.shape)\n",
    "    vector_list.append(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(366.5707)\n",
      "tensor(416.8451)\n",
      "tensor(484.6915)\n",
      "\n",
      "tensor(357.6090)\n",
      "tensor(416.8451)\n",
      "tensor(375.3407)\n"
     ]
    }
   ],
   "source": [
    "def euclidean(x:torch.tensor, y:torch.tensor)->torch.tensor:\n",
    "    return torch.sqrt(torch.pow(x-y,2)).sum()\n",
    "\n",
    "print(euclidean(vector_list[0],vector_list[1]))\n",
    "print(euclidean(vector_list[0],vector_list[2]))\n",
    "print(euclidean(vector_list[0],vector_list[3]))\n",
    "print()\n",
    "print(euclidean(vector_list[2],vector_list[3]))\n",
    "print(euclidean(vector_list[2],vector_list[0]))\n",
    "print(euclidean(vector_list[2],vector_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[126.6876]])\n",
      "tensor([[2.3621]])\n",
      "tensor([[5.8941]])\n",
      "\n",
      "tensor([[52.9290]])\n",
      "tensor([[6.7152]])\n",
      "tensor([[1.6669]])\n"
     ]
    }
   ],
   "source": [
    "def cos_sim(x:torch.tensor, y:torch.tensor)->torch.tensor:\n",
    "    return x@y.T / torch.norm(x)*torch.norm(y)\n",
    "    \n",
    "print(cos_sim(vector_list[0],vector_list[1]))\n",
    "print(cos_sim(vector_list[0],vector_list[2]))\n",
    "print(cos_sim(vector_list[0],vector_list[3]))\n",
    "print()\n",
    "print(cos_sim(vector_list[2],vector_list[3]))\n",
    "print(cos_sim(vector_list[2],vector_list[0]))\n",
    "print(cos_sim(vector_list[2],vector_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2361)\n",
      "tensor(2.8284)\n",
      "tensor([[6.]])\n"
     ]
    }
   ],
   "source": [
    "# print(torch.pow(torch.tensor([4,2]),2))\n",
    "# print(torch.sqrt(torch.tensor([4,2])))\n",
    "# print(torch.tensor([4,2]).sum())\n",
    "x = torch.tensor([[1.,2.]])\n",
    "y = torch.tensor([[2.,2.]])\n",
    "print(torch.norm(x))\n",
    "print(torch.norm(y))\n",
    "# print(y.shape)\n",
    "# print(y.view(-1).shape)\n",
    "# print(x.view(-1).dot(y.view(-1)))\n",
    "print(x@y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.)\n",
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.,2.])\n",
    "y = torch.tensor([2.,2.])\n",
    "print(x@y.T)\n",
    "print(x.dot(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "# model = models.resnet18(models.ResNet18_Weights.DEFAULT)\n",
    "# print(\"params: {:,}\".format(sum([p.data.nelement() for p in model.parameters()])))\n",
    "# x = torch.randn((3,3,224,224))\n",
    "x = np.random.random((3,3,224,224))\n",
    "print(torch.from_numpy(x).shape)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3088, 2316, 3)\n",
      "3088\n"
     ]
    }
   ],
   "source": [
    "print(np_img.shape)\n",
    "print(max(np_img.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([1,2,4,2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da752e93623bd4d3b95b4c8e810fe38ac14da3a724e597d79a95918da767fdb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
