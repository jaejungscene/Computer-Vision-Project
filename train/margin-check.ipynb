{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arcface vs. Cosface vs. Arcface+Cosface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 6, 0, 5, 5, 3])\n",
      "tensor([0, 1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import linear, normalize\n",
    "\n",
    "labels = torch.randint(low=0, high=10, size=(6,))\n",
    "print(labels)\n",
    "index = torch.where(labels!=-1)[0]\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0016, -0.0394, -0.0146,  0.0258, -0.0268],\n",
      "        [ 0.0209, -0.0399,  0.0914, -0.0066,  0.0424],\n",
      "        [-0.0661, -0.0771,  0.0389, -0.0413, -0.0049],\n",
      "        [-0.0681,  0.0132, -0.0275,  0.0343,  0.0055],\n",
      "        [-0.0756, -0.0029,  0.0225, -0.0044, -0.0222],\n",
      "        [-0.0753, -0.0096,  0.1256,  0.0286, -0.0812]],\n",
      "       grad_fn=<ClampBackward1>)\n",
      "torch.Size([6, 5])\n",
      "\n",
      "tensor([1, 0, 0, 2, 4, 1])\n",
      "tensor([0, 1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 6\n",
    "num_class = 5\n",
    "\n",
    "weights = torch.nn.Parameter(torch.FloatTensor(num_class, 512))\n",
    "torch.nn.init.xavier_uniform_(weights)\n",
    "embed_vec = torch.randn((batch_size,512))\n",
    "logits = linear(normalize(embed_vec), normalize(weights)).clamp(-1,1)\n",
    "print(logits)\n",
    "print(logits.shape)\n",
    "print()\n",
    "\n",
    "labels = torch.randint(low=0, high=num_class, size=(batch_size,))\n",
    "print(labels)\n",
    "index = torch.where(labels!=-1)[0]\n",
    "print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0394,  0.0209, -0.0661, -0.0275, -0.0222, -0.0096],\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "target = logits[index, labels[index].view(-1)]\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0159, -5.1361, -0.1460,  0.2579, -0.2678],\n",
       "        [-4.6100, -0.3994,  0.9139, -0.0662,  0.4240],\n",
       "        [-5.3635, -0.7706,  0.3888, -0.4132, -0.0485],\n",
       "        [-0.6811,  0.1322, -5.0339,  0.3428,  0.0551],\n",
       "        [-0.7555, -0.0288,  0.2246, -0.0440, -4.9878],\n",
       "        [-0.7528, -4.8778,  1.2565,  0.2857, -0.8119]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Arcface(0.5)\n",
    "from marginloss import CombinedMarginLoss\n",
    "softmax = CombinedMarginLoss(s=10, m1=1.0, m2=0.5, m3=0.0)\n",
    "softmax(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0159, -8.6361, -0.1460,  0.2579, -0.2678],\n",
       "        [-8.1100, -0.3994,  0.9139, -0.0662,  0.4240],\n",
       "        [-8.8635, -0.7706,  0.3888, -0.4132, -0.0485],\n",
       "        [-0.6811,  0.1322, -8.5339,  0.3428,  0.0551],\n",
       "        [-0.7555, -0.0288,  0.2246, -0.0440, -8.4878],\n",
       "        [-0.7528, -8.3778,  1.2565,  0.2857, -0.8119]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cosface(0.35)\n",
    "softmax = CombinedMarginLoss(s=10, m1=0.0, m2=0.0, m3=0.35)\n",
    "softmax(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.0159, -13.4960,  -0.1460,   0.2579,  -0.2678],\n",
       "        [-13.4221,  -0.3994,   0.9139,  -0.0662,   0.4240],\n",
       "        [-14.7606,  -0.7706,   0.3888,  -0.4132,  -0.0485],\n",
       "        [ -0.6811,   0.1322, -13.4883,   0.3428,   0.0551],\n",
       "        [ -0.7555,  -0.0288,   0.2246,  -0.0440, -13.4837],\n",
       "        [ -0.7528, -13.4699,   1.2565,   0.2857,  -0.8119]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Arcface(0.5) + Cosface(0.35)\n",
    "softmax = CombinedMarginLoss(s=10, m1=0.5, m2=0.5, m3=0.35)\n",
    "softmax(logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from marginloss import CombinedMarginLoss\n",
    "softmax = CombinedMarginLoss(s=2, m1=1.0, m2=0.5, m3=0.0)\n",
    "softmax(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset length:  5,822,653\n",
      "valid dataset length:  13,233\n",
      "485222\n",
      "1103\n"
     ]
    }
   ],
   "source": [
    "from dataset import get_dataloader\n",
    "\n",
    "train_loader, x, valid_loader, y = get_dataloader(local_rank=0, batch_size=12)\n",
    "print(len(train_loader))\n",
    "print(len(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> the number of model parameters: 24,025,600\n"
     ]
    }
   ],
   "source": [
    "from model import get_model\n",
    "ROOT_DIR = \"/home/ljj0512/private/workspace/CV-project/Computer-Vision-Project/train\"\n",
    "model = get_model(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 3, 112, 112])\n",
      "torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "for input, labels in train_loader:\n",
    "    print(input.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 112, 112])\n",
      "torch.Size([6])\n",
      "\n",
      "torch.Size([6, 512])\n",
      "torch.Size([6, 85742])\n",
      "torch.Size([6])\n",
      "46.125762939453125\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn((6,3,112,112))\n",
    "labels = torch.randint(low=0,high=100,size=(6,))\n",
    "print(inputs.shape)\n",
    "print(labels.shape)\n",
    "print()\n",
    "\n",
    "margin_loss = CombinedMarginLoss(64, 1.0, 0.5, 0.0)\n",
    "fc_softmax = FCSoftmax(margin_loss, 512, 85742)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# with torch.no_grad():\n",
    "model.train()\n",
    "fc_softmax.train()\n",
    "embed_vec = model(inputs)\n",
    "print(embed_vec.shape)\n",
    "logits = fc_softmax(embed_vec, labels)\n",
    "print(logits.shape)\n",
    "_, predicted = torch.max(logits.data, dim=1)\n",
    "print(predicted.shape)\n",
    "loss = criterion(logits, labels)\n",
    "print(loss.item())\n",
    "print(\"finish\")\n",
    "# model = PartialFC(margin_loss, 512, 93431, 1.0, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> the number of model parameters: 43,899,904\n"
     ]
    }
   ],
   "source": [
    "print('=> the number of model parameters: {:,}'.format(sum([p.data.nelement() for p in fc_softmax.parameters()])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import collections\n",
    "from typing import Callable\n",
    "from torch import distributed\n",
    "from torch.nn.functional import linear, normalize\n",
    "\n",
    "class PartialFC(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/2203.15565\n",
    "    A distributed sparsely updating variant of the FC layer, named Partial FC (PFC).\n",
    "    When sample rate less than 1, in each iteration, positive class centers and a random subset of\n",
    "    negative class centers are selected to compute the margin-based softmax loss, all class\n",
    "    centers are still maintained throughout the whole training process, but only a subset is\n",
    "    selected and updated in each iteration.\n",
    "    .. note::\n",
    "        When sample rate equal to 1, Partial FC is equal to model parallelism(default sample rate is 1).\n",
    "    Example:\n",
    "    --------\n",
    "    >>> module_pfc = PartialFC(embedding_size=512, num_classes=8000000, sample_rate=0.2)\n",
    "    >>> for img, labels in data_loader:\n",
    "    >>>     embeddings = net(img)\n",
    "    >>>     loss = module_pfc(embeddings, labels, optimizer)\n",
    "    >>>     loss.backward()\n",
    "    >>>     optimizer.step()\n",
    "    \"\"\"\n",
    "    _version = 1 \n",
    "    def __init__(\n",
    "        self,\n",
    "        margin_loss: Callable,\n",
    "        embedding_size: int,\n",
    "        num_classes: int,\n",
    "        sample_rate: float = 1.0,\n",
    "        fp16: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Paramenters:\n",
    "        -----------\n",
    "        embedding_size: int\n",
    "            The dimension of embedding, required\n",
    "        num_classes: int\n",
    "            Total number of classes, required\n",
    "        sample_rate: float\n",
    "            The rate of negative centers participating in the calculation, default is 1.0.\n",
    "        \"\"\"\n",
    "        super(PartialFC, self).__init__()\n",
    "        # assert (\n",
    "        #     distributed.is_initialized()\n",
    "        # ), \"must initialize distributed before create this\"\n",
    "        self.rank = distributed.get_rank()\n",
    "        self.world_size = distributed.get_world_size()\n",
    "\n",
    "        self.dist_cross_entropy = DistCrossEntropy()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.sample_rate: float = sample_rate\n",
    "        self.fp16 = fp16\n",
    "        self.num_local: int = num_classes // self.world_size + int(\n",
    "            self.rank < num_classes % self.world_size\n",
    "        )\n",
    "        self.class_start: int = num_classes // self.world_size * self.rank + min(\n",
    "            self.rank, num_classes % self.world_size\n",
    "        )\n",
    "        self.num_sample: int = int(self.sample_rate * self.num_local)\n",
    "        self.last_batch_size: int = 0\n",
    "        self.weight: torch.Tensor\n",
    "        self.weight_mom: torch.Tensor\n",
    "        self.weight_activated: torch.nn.Parameter\n",
    "        self.weight_activated_mom: torch.Tensor\n",
    "        self.is_updated: bool = True\n",
    "        self.init_weight_update: bool = True\n",
    "\n",
    "        if self.sample_rate < 1:\n",
    "            self.register_buffer(\"weight\",\n",
    "                tensor=torch.normal(0, 0.01, (self.num_local, embedding_size)))\n",
    "            self.register_buffer(\"weight_mom\",\n",
    "                tensor=torch.zeros_like(self.weight))\n",
    "            self.register_parameter(\"weight_activated\",\n",
    "                param=torch.nn.Parameter(torch.empty(0, 0)))\n",
    "            self.register_buffer(\"weight_activated_mom\",\n",
    "                tensor=torch.empty(0, 0))\n",
    "            self.register_buffer(\"weight_index\",\n",
    "                tensor=torch.empty(0, 0))\n",
    "        else:\n",
    "            self.weight_activated = torch.nn.Parameter(torch.normal(0, 0.01, (self.num_local, embedding_size)))\n",
    "\n",
    "        # margin_loss\n",
    "        if isinstance(margin_loss, Callable):\n",
    "            self.margin_softmax = margin_loss\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, \n",
    "        labels: torch.Tensor, \n",
    "        index_positive: torch.Tensor, \n",
    "        optimizer: torch.optim.Optimizer):\n",
    "        \"\"\"\n",
    "        This functions will change the value of labels\n",
    "        Parameters:\n",
    "        -----------\n",
    "        labels: torch.Tensor\n",
    "            pass\n",
    "        index_positive: torch.Tensor\n",
    "            pass\n",
    "        optimizer: torch.optim.Optimizer\n",
    "            pass\n",
    "        \"\"\"\n",
    "        positive = torch.unique(labels[index_positive], sorted=True).cuda()\n",
    "        if self.num_sample - positive.size(0) >= 0:\n",
    "            perm = torch.rand(size=[self.num_local]).cuda()\n",
    "            perm[positive] = 2.0\n",
    "            index = torch.topk(perm, k=self.num_sample)[1].cuda()\n",
    "            index = index.sort()[0].cuda()\n",
    "        else:\n",
    "            index = positive\n",
    "        self.weight_index = index\n",
    "\n",
    "        labels[index_positive] = torch.searchsorted(index, labels[index_positive])\n",
    "        \n",
    "        self.weight_activated = torch.nn.Parameter(self.weight[self.weight_index])\n",
    "        self.weight_activated_mom = self.weight_mom[self.weight_index]\n",
    "        \n",
    "        if isinstance(optimizer, torch.optim.SGD):\n",
    "            # TODO the params of partial fc must be last in the params list\n",
    "            optimizer.state.pop(optimizer.param_groups[-1][\"params\"][0], None)\n",
    "            optimizer.param_groups[-1][\"params\"][0] = self.weight_activated\n",
    "            optimizer.state[self.weight_activated][\n",
    "                \"momentum_buffer\"\n",
    "            ] = self.weight_activated_mom\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self):\n",
    "        \"\"\" partial weight to global\n",
    "        \"\"\"\n",
    "        if self.init_weight_update:\n",
    "            self.init_weight_update = False\n",
    "            return\n",
    "\n",
    "        if self.sample_rate < 1:\n",
    "            self.weight[self.weight_index] = self.weight_activated\n",
    "            self.weight_mom[self.weight_index] = self.weight_activated_mom\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        local_embeddings: torch.Tensor,\n",
    "        local_labels: torch.Tensor,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        ----------\n",
    "        local_embeddings: torch.Tensor\n",
    "            feature embeddings on each GPU(Rank).\n",
    "        local_labels: torch.Tensor\n",
    "            labels on each GPU(Rank).\n",
    "        Returns:\n",
    "        -------\n",
    "        loss: torch.Tensor\n",
    "            pass\n",
    "        \"\"\"\n",
    "        local_labels.squeeze_()\n",
    "        local_labels = local_labels.long()\n",
    "        self.update()\n",
    "\n",
    "        batch_size = local_embeddings.size(0)\n",
    "        if self.last_batch_size == 0:\n",
    "            self.last_batch_size = batch_size\n",
    "        assert self.last_batch_size == batch_size, (\n",
    "            \"last batch size do not equal current batch size: {} vs {}\".format(\n",
    "            self.last_batch_size, batch_size))\n",
    "\n",
    "        _gather_embeddings = [\n",
    "            torch.zeros((batch_size, self.embedding_size)).cuda()\n",
    "            for _ in range(self.world_size)\n",
    "        ]\n",
    "        _gather_labels = [\n",
    "            torch.zeros(batch_size).long().cuda() for _ in range(self.world_size)\n",
    "        ]\n",
    "        _list_embeddings = AllGather(local_embeddings, *_gather_embeddings)\n",
    "        distributed.all_gather(_gather_labels, local_labels)\n",
    "\n",
    "        embeddings = torch.cat(_list_embeddings)\n",
    "        labels = torch.cat(_gather_labels)\n",
    "\n",
    "        labels = labels.view(-1, 1)\n",
    "        index_positive = (self.class_start <= labels) & (\n",
    "            labels < self.class_start + self.num_local\n",
    "        )\n",
    "        labels[~index_positive] = -1\n",
    "        labels[index_positive] -= self.class_start\n",
    "\n",
    "        if self.sample_rate < 1:\n",
    "            self.sample(labels, index_positive, optimizer)\n",
    "\n",
    "        with torch.cuda.amp.autocast(self.fp16):\n",
    "            norm_embeddings = normalize(embeddings)\n",
    "            norm_weight_activated = normalize(self.weight_activated)\n",
    "            logits = linear(norm_embeddings, norm_weight_activated)\n",
    "        if self.fp16:\n",
    "            logits = logits.float()\n",
    "        logits = logits.clamp(-1, 1)\n",
    "\n",
    "        logits = self.margin_softmax(logits, labels)\n",
    "        loss = self.dist_cross_entropy(logits, labels)\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self, destination=None, prefix=\"\", keep_vars=False):\n",
    "        if destination is None: \n",
    "            destination = collections.OrderedDict()\n",
    "            destination._metadata = collections.OrderedDict()\n",
    "\n",
    "        for name, module in self._modules.items():\n",
    "            if module is not None:\n",
    "                module.state_dict(destination, prefix + name + \".\", keep_vars=keep_vars)\n",
    "        if self.sample_rate < 1:\n",
    "            destination[\"weight\"] = self.weight.detach()\n",
    "        else:\n",
    "            destination[\"weight\"] = self.weight_activated.data.detach()\n",
    "        return destination\n",
    "\n",
    "    def load_state_dict(self, state_dict, strict: bool = True):\n",
    "        if self.sample_rate < 1:\n",
    "            self.weight = state_dict[\"weight\"].to(self.weight.device)\n",
    "            self.weight_mom.zero_()\n",
    "            self.weight_activated.data.zero_()\n",
    "            self.weight_activated_mom.zero_()\n",
    "            self.weight_index.zero_()\n",
    "        else:\n",
    "            self.weight_activated.data = state_dict[\"weight\"].to(self.weight_activated.data.device)\n",
    "\n",
    "class DistCrossEntropyFunc(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    CrossEntropy loss is calculated in parallel, allreduce denominator into single gpu and calculate softmax.\n",
    "    Implemented of ArcFace (https://arxiv.org/pdf/1801.07698v1.pdf):\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, logits: torch.Tensor, label: torch.Tensor):\n",
    "        \"\"\" \"\"\"\n",
    "        batch_size = logits.size(0)\n",
    "        # for numerical stability\n",
    "        max_logits, _ = torch.max(logits, dim=1, keepdim=True)\n",
    "        # local to global\n",
    "        distributed.all_reduce(max_logits, distributed.ReduceOp.MAX)\n",
    "        logits.sub_(max_logits)\n",
    "        logits.exp_()\n",
    "        sum_logits_exp = torch.sum(logits, dim=1, keepdim=True)\n",
    "        # local to global\n",
    "        distributed.all_reduce(sum_logits_exp, distributed.ReduceOp.SUM)\n",
    "        logits.div_(sum_logits_exp)\n",
    "        index = torch.where(label != -1)[0]\n",
    "        # loss\n",
    "        loss = torch.zeros(batch_size, 1, device=logits.device)\n",
    "        loss[index] = logits[index].gather(1, label[index])\n",
    "        distributed.all_reduce(loss, distributed.ReduceOp.SUM)\n",
    "        ctx.save_for_backward(index, logits, label)\n",
    "        return loss.clamp_min_(1e-30).log_().mean() * (-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, loss_gradient):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            loss_grad (torch.Tensor): gradient backward by last layer\n",
    "        Returns:\n",
    "            gradients for each input in forward function\n",
    "            `None` gradients for one-hot label\n",
    "        \"\"\"\n",
    "        (\n",
    "            index,\n",
    "            logits,\n",
    "            label,\n",
    "        ) = ctx.saved_tensors\n",
    "        batch_size = logits.size(0)\n",
    "        one_hot = torch.zeros(\n",
    "            size=[index.size(0), logits.size(1)], device=logits.device\n",
    "        )\n",
    "        one_hot.scatter_(1, label[index], 1)\n",
    "        logits[index] -= one_hot\n",
    "        logits.div_(batch_size)\n",
    "        return logits * loss_gradient.item(), None\n",
    "\n",
    "\n",
    "class DistCrossEntropy(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistCrossEntropy, self).__init__()\n",
    "\n",
    "    def forward(self, logit_part, label_part):\n",
    "        return DistCrossEntropyFunc.apply(logit_part, label_part)\n",
    "\n",
    "\n",
    "class AllGatherFunc(torch.autograd.Function):\n",
    "    \"\"\"AllGather op with gradient backward\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor, *gather_list):\n",
    "        gather_list = list(gather_list)\n",
    "        distributed.all_gather(gather_list, tensor)\n",
    "        return tuple(gather_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grads):\n",
    "        grad_list = list(grads)\n",
    "        rank = distributed.get_rank()\n",
    "        grad_out = grad_list[rank]\n",
    "\n",
    "        dist_ops = [\n",
    "            distributed.reduce(grad_out, rank, distributed.ReduceOp.SUM, async_op=True)\n",
    "            if i == rank\n",
    "            else distributed.reduce(\n",
    "                grad_list[i], i, distributed.ReduceOp.SUM, async_op=True\n",
    "            )\n",
    "            for i in range(distributed.get_world_size())\n",
    "        ]\n",
    "        for _op in dist_ops:\n",
    "            _op.wait()\n",
    "\n",
    "        grad_out *= len(grad_list)  # cooperate with distributed loss function\n",
    "        return (grad_out, *[None for _ in range(len(grad_list))])\n",
    "\n",
    "\n",
    "AllGather = AllGatherFunc.apply\n",
    "# Footer\n",
    "# © 2022 GitHub, Inc.\n",
    "# Footer navigation\n",
    "# Terms\n",
    "# Privacy\n",
    "# Security\n",
    "# Status\n",
    "# Docs\n",
    "# Contact GitHub\n",
    "# Pricing\n",
    "# API\n",
    "# Training\n",
    "# Blog\n",
    "# About\n",
    "# insightface/partial_fc.py at master · deepinsight/insightface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Callable\n",
    "from torch.nn.functional import linear, normalize\n",
    "\n",
    "class FCSoftmax(nn.Module):\n",
    "    def __init__(self, margin_softmax: Callable, embed_size: int, num_classes: int):\n",
    "        super(FCSoftmax, self).__init__()\n",
    "        self.margin_softmax = margin_softmax\n",
    "        self.weights = nn.Parameter(torch.FloatTensor(num_classes, embed_size))\n",
    "        nn.init.xavier_uniform_(self.weights)\n",
    "    \n",
    "    def forward(self, embed_vec: torch.Tensor, labels: torch.Tensor):\n",
    "        logits = linear(normalize(embed_vec), normalize(self.weights)).clamp(-1,1)\n",
    "        logits = self.margin_softmax(logits, labels)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class CombinedMarginLoss(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 s, \n",
    "                 m1,\n",
    "                 m2,\n",
    "                 m3,\n",
    "                 interclass_filtering_threshold=0):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "        self.m1 = m1\n",
    "        self.m2 = m2\n",
    "        self.m3 = m3\n",
    "        self.interclass_filtering_threshold = interclass_filtering_threshold\n",
    "        \n",
    "        # For ArcFace\n",
    "        self.cos_m = math.cos(self.m2)\n",
    "        self.sin_m = math.sin(self.m2)\n",
    "        self.theta = math.cos(math.pi - self.m2)\n",
    "        self.sinmm = math.sin(math.pi - self.m2) * self.m2\n",
    "        self.easy_margin = False\n",
    "\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        index_positive = torch.where(labels != -1)[0]\n",
    "\n",
    "        if self.interclass_filtering_threshold > 0:\n",
    "            with torch.no_grad():\n",
    "                dirty = logits > self.interclass_filtering_threshold\n",
    "                dirty = dirty.float()\n",
    "                mask = torch.ones([index_positive.size(0), logits.size(1)], device=logits.device)\n",
    "                mask.scatter_(1, labels[index_positive], 0)\n",
    "                dirty[index_positive] *= mask\n",
    "                tensor_mul = 1 - dirty    \n",
    "            logits = tensor_mul * logits\n",
    "\n",
    "        target_logit = logits[index_positive, labels[index_positive].view(-1)]\n",
    "\n",
    "        if self.m1 == 1.0 and self.m3 == 0.0:\n",
    "            sin_theta = torch.sqrt(1.0 - torch.pow(target_logit, 2))\n",
    "            cos_theta_m = target_logit * self.cos_m - sin_theta * self.sin_m  # cos(target+margin)\n",
    "            if self.easy_margin:\n",
    "                final_target_logit = torch.where(\n",
    "                    target_logit > 0, cos_theta_m, target_logit)\n",
    "            else:\n",
    "                final_target_logit = torch.where(\n",
    "                    target_logit > self.theta, cos_theta_m, target_logit - self.sinmm)\n",
    "            logits[index_positive, labels[index_positive].view(-1)] = final_target_logit\n",
    "            logits = logits * self.s\n",
    "        \n",
    "        elif self.m3 > 0:\n",
    "            final_target_logit = target_logit - self.m3\n",
    "            logits[index_positive, labels[index_positive].view(-1)] = final_target_logit\n",
    "            logits = logits * self.s\n",
    "        else:\n",
    "            raise        \n",
    "\n",
    "        return logits\n",
    "\n",
    "class ArcFace(torch.nn.Module):\n",
    "    \"\"\" ArcFace (https://arxiv.org/pdf/1801.07698v1.pdf):\n",
    "    \"\"\"\n",
    "    def __init__(self, s=64.0, margin=0.5):\n",
    "        super(ArcFace, self).__init__()\n",
    "        self.scale = s\n",
    "        self.cos_m = math.cos(margin)\n",
    "        self.sin_m = math.sin(margin)\n",
    "        self.theta = math.cos(math.pi - margin)\n",
    "        self.sinmm = math.sin(math.pi - margin) * margin\n",
    "        self.easy_margin = False\n",
    "\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, labels: torch.Tensor):\n",
    "        index = torch.where(labels != -1)[0]\n",
    "        target_logit = logits[index, labels[index].view(-1)]\n",
    "\n",
    "        sin_theta = torch.sqrt(1.0 - torch.pow(target_logit, 2))\n",
    "        cos_theta_m = target_logit * self.cos_m - sin_theta * self.sin_m  # cos(target+margin)\n",
    "        if self.easy_margin:\n",
    "            final_target_logit = torch.where(\n",
    "                target_logit > 0, cos_theta_m, target_logit)\n",
    "        else:\n",
    "            final_target_logit = torch.where(\n",
    "                target_logit > self.theta, cos_theta_m, target_logit - self.sinmm)\n",
    "\n",
    "        logits[index, labels[index].view(-1)] = final_target_logit\n",
    "        logits = logits * self.scale\n",
    "        return logits\n",
    "\n",
    "\n",
    "class CosFace(torch.nn.Module):\n",
    "    def __init__(self, s=64.0, m=0.40):\n",
    "        super(CosFace, self).__init__()\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, labels: torch.Tensor):\n",
    "        index = torch.where(labels != -1)[0]\n",
    "        target_logit = logits[index, labels[index].view(-1)]\n",
    "        final_target_logit = target_logit - self.m\n",
    "        logits[index, labels[index].view(-1)] = final_target_logit\n",
    "        logits = logits * self.s\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Callable\n",
    "from torch.nn.functional import linear, normalize\n",
    "\n",
    "class FCSoftmax(nn.Module):\n",
    "    def __init__(self, margin_softmax: Callable, embed_size: int, num_classes: int):\n",
    "        super(FCSoftmax, self).__init__()\n",
    "        self.margin_softmax = margin_softmax\n",
    "        self.weights = torch.nn.Parameter(torch.FloatTensor(num_classes, embed_size))\n",
    "        nn.init.xavier_uniform_(self.weights)\n",
    "    \n",
    "    def forward(self, embed_vec: torch.Tensor, labels: torch.Tensor):\n",
    "        logits = linear(normalize(embed_vec), normalize(self.weights)).clamp(-1,1)\n",
    "        logits = self.margin_softmax(logits, labels)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed.is_initialized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m margin_loss \u001b[39m=\u001b[39m CombinedMarginLoss(\u001b[39m64\u001b[39m, \u001b[39m1.0\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.0\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m PartialFC(margin_loss, \u001b[39m512\u001b[39;49m, \u001b[39m93431\u001b[39;49m, \u001b[39m1.0\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn [7], line 49\u001b[0m, in \u001b[0;36mPartialFC.__init__\u001b[0;34m(self, margin_loss, embedding_size, num_classes, sample_rate, fp16)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39msuper\u001b[39m(PartialFC, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     46\u001b[0m \u001b[39m# assert (\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m#     distributed.is_initialized()\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m# ), \"must initialize distributed before create this\"\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrank \u001b[39m=\u001b[39m distributed\u001b[39m.\u001b[39;49mget_rank()\n\u001b[1;32m     50\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworld_size \u001b[39m=\u001b[39m distributed\u001b[39m.\u001b[39mget_world_size()\n\u001b[1;32m     52\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_cross_entropy \u001b[39m=\u001b[39m DistCrossEntropy()\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:844\u001b[0m, in \u001b[0;36mget_rank\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[39mif\u001b[39;00m _rank_not_in_group(group):\n\u001b[1;32m    842\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m--> 844\u001b[0m default_pg \u001b[39m=\u001b[39m _get_default_group()\n\u001b[1;32m    845\u001b[0m \u001b[39mif\u001b[39;00m group \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m group \u001b[39mis\u001b[39;00m GroupMember\u001b[39m.\u001b[39mWORLD:\n\u001b[1;32m    846\u001b[0m     \u001b[39mreturn\u001b[39;00m default_pg\u001b[39m.\u001b[39mrank()\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:429\u001b[0m, in \u001b[0;36m_get_default_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39mGetting the default process group created by init_process_group\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_initialized():\n\u001b[0;32m--> 429\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    430\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDefault process group has not been initialized, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mplease make sure to call init_process_group.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m     )\n\u001b[1;32m    433\u001b[0m \u001b[39mreturn\u001b[39;00m GroupMember\u001b[39m.\u001b[39mWORLD\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "margin_loss = CombinedMarginLoss(64, 1.0, 0.5, 0.0)\n",
    "\n",
    "model = PartialFC(margin_loss, 512, 93431, 1.0, True)\n",
    "\n",
    "# model = torch.nn.Parameter(x)\n",
    "# print(model.shape)\n",
    "\n",
    "# print('=> the number of model parameters: {:,}'.format(sum([p.data.nelement() for p in model.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65fe116ec29312474b580f4ecbad52a94f46ea3a142b15e85ff8e68848a207e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
